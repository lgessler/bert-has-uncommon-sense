{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.anaconda3/envs/sembre/lib/python3.8/site-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "from bssp.common.reading import read_dataset_cached, indexer_for_embedder, embedder_for_embedding\n",
    "from bssp.common.embedder_model import EmbedderModel, EmbedderModelPredictor, EmbedderDatasetReader\n",
    "from allennlp.data import Token\n",
    "\n",
    "def activate_bert_layers(embedder, layers):\n",
    "    \"\"\"\n",
    "    The Embedder has params deep inside that produce a scalar mix of BERT layers via a softmax\n",
    "    followed by a dot product. Activate the ones specified in `layers` and deactivate the rest\n",
    "    \"\"\"\n",
    "    # whew!\n",
    "    scalar_mix = embedder.token_embedder_tokens._matched_embedder._scalar_mix.scalar_parameters\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, param in enumerate(scalar_mix):\n",
    "            param.requires_grad = False\n",
    "            param.fill_(1e9 if i in layers else -1e9)\n",
    "        \n",
    "indexer = indexer_for_embedder('bert-base-cased')\n",
    "vocab, embedder = embedder_for_embedding('bert-base-cased')\n",
    "reader = EmbedderDatasetReader({\"tokens\": indexer})\n",
    "model = EmbedderModel(vocab, embedder).to('cuda').eval()\n",
    "predictor = EmbedderModelPredictor(model, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Luke', \"'\", 's', 'the', 'one', 'who', 'wrote', 'this', 'sentence', '!', '[SEP]']\n",
      "Layer 1 and layer 1: 1.0\n",
      "Layer 1 and layer 2: 0.8992964625358582\n",
      "Layer 1 and layer 3: 0.794964611530304\n",
      "Layer 1 and layer 4: 0.6988664865493774\n",
      "Layer 1 and layer 5: 0.5934978723526001\n",
      "Layer 1 and layer 6: 0.5190457701683044\n",
      "Layer 1 and layer 7: 0.44594624638557434\n",
      "Layer 1 and layer 8: 0.40702757239341736\n",
      "Layer 1 and layer 9: 0.35980919003486633\n",
      "Layer 1 and layer 10: 0.34618818759918213\n",
      "Layer 1 and layer 11: 0.3302433490753174\n",
      "Layer 1 and layer 12: 0.27428197860717773\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def process_sent_allennlp(text, pos, ref_layer):\n",
    "    embs = []\n",
    "    for i in range(0, 12):\n",
    "        activate_bert_layers(embedder, [i])\n",
    "        with torch.no_grad():\n",
    "            res = predictor.predict(text.split())\n",
    "        if i == 0:\n",
    "            print([vocab.get_token_from_index(t, \"tokens\") for t in res['token_ids']])\n",
    "        embeddings = torch.tensor(res['embeddings'])\n",
    "        embs.append(embeddings[pos])\n",
    "\n",
    "    first_emb = embs[ref_layer]\n",
    "    first_emb.unsqueeze_(0)\n",
    "    for i, emb in enumerate(embs):\n",
    "        if i != ref_layer:\n",
    "            emb.unsqueeze_(0)\n",
    "        print(f\"Layer {ref_layer+1} and layer {i+1}:\", F.cosine_similarity(first_emb, emb).item())\n",
    "\n",
    "process_sent_allennlp(\"Luke 's the one who wrote this sentence !\", 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it with transformers now\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "# GPU available?\n",
    "t_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "t_config = BertConfig.from_pretrained(\"bert-base-cased\", output_hidden_states=True)\n",
    "t_model = BertModel.from_pretrained('bert-base-cased', config=t_config).to('cuda:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Luke', 'is', 'the', 'one', 'who', 'wrote', 'this', '!', '[SEP]']\n",
      "Layer 1 and layer 1: 1.0\n",
      "Layer 1 and layer 2: 0.9288332462310791\n",
      "Layer 1 and layer 3: 0.8593798279762268\n",
      "Layer 1 and layer 4: 0.7701111435890198\n",
      "Layer 1 and layer 5: 0.7190108299255371\n",
      "Layer 1 and layer 6: 0.6400652527809143\n",
      "Layer 1 and layer 7: 0.6084674596786499\n",
      "Layer 1 and layer 8: 0.5701310634613037\n",
      "Layer 1 and layer 9: 0.4878999888896942\n",
      "Layer 1 and layer 10: 0.4057501256465912\n",
      "Layer 1 and layer 11: 0.3372558355331421\n",
      "Layer 1 and layer 12: 0.3738574981689453\n",
      "\n",
      "['[CLS]', 'Luke', 'is', 'the', 'one', 'who', 'wrote', 'this', '!', '[SEP]']\n",
      "Layer 1 and layer 1: 1.0\n",
      "Layer 1 and layer 2: 0.8729580044746399\n",
      "Layer 1 and layer 3: 0.7460393905639648\n",
      "Layer 1 and layer 4: 0.6788817048072815\n",
      "Layer 1 and layer 5: 0.6552401185035706\n",
      "Layer 1 and layer 6: 0.6503338813781738\n",
      "Layer 1 and layer 7: 0.6055536866188049\n",
      "Layer 1 and layer 8: 0.5832138061523438\n",
      "Layer 1 and layer 9: 0.5874451398849487\n",
      "Layer 1 and layer 10: 0.5963452458381653\n",
      "Layer 1 and layer 11: 0.5698867440223694\n",
      "Layer 1 and layer 12: 0.5715781450271606\n",
      "\n",
      "\n",
      "['[CLS]', 'Ba', '##h', '!', '[SEP]']\n",
      "Layer 1 and layer 1: 1.0\n",
      "Layer 1 and layer 2: 0.8855591416358948\n",
      "Layer 1 and layer 3: 0.782077968120575\n",
      "Layer 1 and layer 4: 0.7223076224327087\n",
      "Layer 1 and layer 5: 0.6699797511100769\n",
      "Layer 1 and layer 6: 0.6100362539291382\n",
      "Layer 1 and layer 7: 0.5547953844070435\n",
      "Layer 1 and layer 8: 0.5131151676177979\n",
      "Layer 1 and layer 9: 0.4842303693294525\n",
      "Layer 1 and layer 10: 0.4612303078174591\n",
      "Layer 1 and layer 11: 0.41578638553619385\n",
      "Layer 1 and layer 12: 0.36232990026474\n",
      "\n",
      "['[CLS]', 'Ba', '##h', '!', '[SEP]']\n",
      "Layer 1 and layer 1: 1.0\n",
      "Layer 1 and layer 2: 0.8886198997497559\n",
      "Layer 1 and layer 3: 0.7550865411758423\n",
      "Layer 1 and layer 4: 0.6064193248748779\n",
      "Layer 1 and layer 5: 0.5240603089332581\n",
      "Layer 1 and layer 6: 0.4660964608192444\n",
      "Layer 1 and layer 7: 0.43539923429489136\n",
      "Layer 1 and layer 8: 0.3985525667667389\n",
      "Layer 1 and layer 9: 0.363737016916275\n",
      "Layer 1 and layer 10: 0.30896976590156555\n",
      "Layer 1 and layer 11: 0.25351542234420776\n",
      "Layer 1 and layer 12: 0.20540554821491241\n"
     ]
    }
   ],
   "source": [
    "def process_sent_transformers(text, pos, ref_layer):\n",
    "    marked_text = '[CLS] ' + text + ' [SEP]'\n",
    "    tokenized_text = t_tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = t_tokenizer.convert_tokens_to_ids(tokenized_text) \n",
    "    print(t_tokenizer.convert_ids_to_tokens(indexed_tokens))\n",
    "    segments_ids = [1] * len(tokenized_text) \n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')\n",
    "    segments_tensor = torch.tensor([segments_ids]).to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = t_model(tokens_tensor, segments_tensor, return_dict=True)\n",
    "    encoded_layers = outputs['hidden_states']\n",
    "        \n",
    "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "    #Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    \n",
    "    first_emb = token_embeddings[ref_layer, pos, :]\n",
    "    first_emb.unsqueeze_(0)\n",
    "    for i, emb in enumerate(token_embeddings[:12, pos, :]):\n",
    "        emb.unsqueeze_(0)\n",
    "        print(f\"Layer {ref_layer+1} and layer {i+1}:\", F.cosine_similarity(first_emb, emb).item())\n",
    "\n",
    "process_sent_allennlp(\"Luke is the one who wrote this !\", pos=0, ref_layer=0)\n",
    "print()\n",
    "process_sent_transformers(\"Luke is the one who wrote this !\", pos=0, ref_layer=0)\n",
    "print()\n",
    "print()\n",
    "process_sent_allennlp(\"Bah !\", pos=1, ref_layer=0)\n",
    "print()\n",
    "process_sent_transformers(\"Bah !\", pos=1, ref_layer=0)\n",
    "\n",
    "# scalar mix matters?! and we still don't get the kinds of differences we get with raw transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
